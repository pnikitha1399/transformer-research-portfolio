# Transformer Research Portfolio

This repository documents my independent research study of Transformer architectures, beginning with the paper:

**"Attention Is All You Need" (Vaswani et al., 2017)**

The goal of this work is to build deep theoretical and empirical understanding of attention-based models and to develop original research artifacts suitable for PhD and pre-doctoral research applications.

---

## Research Philosophy

- Focus on first-principles understanding
- Emphasis on clear assumptions and failure modes
- Reproducible experiments and transparent analysis
- Research > benchmarks

---

## Repository Structure

### `reading-notes/`
Conceptual and critical reading notes of research papers, written in my own words.

### `math/`
Mathematical derivations and shape-level explanations of key model components.

### `experiments/`
Ablation studies, controlled experiments, and replications.

- `attention-variants/`: Softmax alternatives, attention behavior
- `positional-encoding/`: Positional information and generalization
- `head-analysis/`: Multi-head attention redundancy and specialization

### `mini-papers/`
Short research reports (4â€“8 pages) written in academic style.

### `blog/`
Long-form explanatory posts and interpretability analyses.

---

## Current Focus

- Deep reading and analysis of *Attention Is All You Need*
- Component-wise study of Transformer design decisions

---

## Long-Term Goal

To develop a coherent body of research demonstrating:
- Strong theoretical intuition
- Clean experimental design
- Clear technical communication

This repository serves as a living research log and portfolio.
